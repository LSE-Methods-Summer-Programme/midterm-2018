---
title: "Midterm Assignemnt, ME314 2018"
author: "Qin Yu"
output: html_document
---
 
![](images/lse-logo.jpg)

#### Summer School 2018 midsession examination  

# ME314 Introduction to Data Science and Big Data 

## Suitable for all candidates


### Instructions to candidates  

* Complete the assignment by adding your answers directly to the RMarkdown document, knitting the document, and submitting the HTML file to Moodle.   
* Time allowed: due 19:00 on Wednesday, 8th August 2018.  
* Submit the assignment via [Moodle](https://shortcourses.lse.ac.uk/course/view.php?id=158).


You will need to load the core library for the course textbook and libraries for LDA and KNN:
```{r}
library(ISLR)
library(MASS)
library(class)
```

This question should be answered using the `Weekly` data set, which is part of the `ISLR` package. This data contains 1,089 weekly stock returns for 21 years, from the beginning of 1990 to the end of 2010.

1.   Produce some numerical and graphical summaries of the `Weekly` data. Do there appear to be any patterns?

```{r}
data(Weekly)
names(Weekly)
dim(Weekly)
summary(Weekly)
#plot(Weekly)
pairs(Weekly)
cor(subset(Weekly, select = -Direction))
```

**The codes above show the significant correlation between `Year` and `Volume`, which intuitively makes sense, while the other dota does not correlate to each other in an obvious way.**


***


2.  Use the full data set to perform a logistic regression with `Direction` as the response and the five lag variables plus `Volume` as predictors. Use the summary function to print the results. 

    Do any of the predictors appear to be statistically significant? If so, which ones?

```{r}
glm.fits <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly ,family = binomial)
summary(glm.fits)
# The following codes do not give '*'s:
#summary(glm.fits)$coefficients
#summary(glm.fits)$coefficients[, 4]
#coef(glm.fits)
```

**`Lag2` seems to be statistically significant, since it has a relatively small p-value, while all the others' p-values are too big to give a clear evidence of a real association with `Direction`.**


***


3.  Compute the confusion matrix and overall fraction of correct predictions. 
    
    Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

```{r}
contrasts(Weekly$Direction)
glm.probs <- predict(glm.fits, type = "response")
#glm.probs[1:10]
glm.pred <- rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] <- "Up"
table(glm.pred, Weekly$Direction)
mean(glm.pred == Weekly$Direction)
```
     
**In this case, logistic regression correctly predicted the movement of the stock ``r sprintf("%.2f %%", 100*mean(glm.pred == Weekly$Direction))`` of the time. The confusion matrix, with the off-diagonals representing incorrect predictions including false positives and false negatives, is telling me that the training error is high.**


***


4.  Now fit the logistic regression model using a training data period from 1990 to 2008, with `Lag2` as the only predictor. 

```{r}
train <- (Weekly$Year < 2009)
Weekly.test <- Weekly[!train, ]
Weekly.train <- Weekly[train, ]
glm.fits.subset <- glm(Direction ~ Lag2, data = Weekly ,family = binomial, subset = train)
summary(glm.fits.subset)
#glm.fits.train <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly.train ,family = binomial)
#summary(glm.fits.train)
```


  Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).

```{r}
glm.probs.test <- predict(glm.fits.subset, Weekly.test, type = "response")
glm.pred.test <- rep("Down", length(glm.probs.test))
glm.pred.test[glm.probs.test > 0.5] <- "Up"
table(glm.pred.test, Weekly.test$Direction)
mean(glm.pred.test == Weekly.test$Direction)
```

**Here, logistic regression correctly predicted the movement of the stock ``r sprintf("%.2f %%", 100*mean(glm.pred.test == Weekly.test$Direction))`` of the time**


***


5.  Experiment with different combinations of predictors, including possible transformations and interactions, and classification methods. 

    Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data.
    
**Firstly, try all the combinations of predictors:**

```{r}
predictors <- names(Weekly)[2: 7]
# Getting all combinations of predictors:
boolcombinations <- expand.grid(rep(list(c(TRUE, FALSE)), length(predictors)))
boolcombinations <- boolcombinations[-(dim(boolcombinations)[1]), ]
combinations <- apply(boolcombinations, 1, function(x) as.formula(
  paste("Direction ~ ", paste(predictors[x], collapse = " + "), sep ="")
  ))
# Getting all glm:
allresults <- lapply(combinations, function(x) glm(x, data = Weekly ,family = binomial))
allresults[[1]]$aic  # Verify that this AIC has the same value as in Q2
min(as.numeric(lapply(allresults, function(x) x$aic)))
# Finding the best combination:
wantedcombination <- which.min(as.numeric(lapply(allresults, function(x) x$aic)))
combinations[wantedcombination]
```

**So, according to AIC, the best combination of predictors is ``r predictors[unlist(boolcombinations[wantedcombination, ])]``, for logistic regression.**

**Now, try to find the best combination regarding the error rate, with data prior to 2009 as training set:**

```{r}
allresults.train <- lapply(combinations, function(x) glm(x, data = Weekly ,family = binomial, subset = train))
allprobs <- lapply(allresults, function(x) predict(x, Weekly.test, type = "response"))
allpred <- vector("list", length = length(allprobs))
#allpred <- lapply(allpred, function(x) unlist(x))
#allpred[1] <- list(rep("Down", dim(Weekly.test)[1]))
allpred[c(1:length(allpred))] <- list(rep("Down", dim(Weekly.test)[1]))
for (i in 1:length(allpred)) {
  for (j in 1:dim(Weekly.test)[1]) {
    if (allprobs[[i]][j] > 0.5) {
      allpred[[i]][j] <- "Up"
    }
  }
}
allmean <- lapply(allpred, function(x) mean(x == Weekly.test$Direction))
allmean[1]  # This should be the same as Q4, verified.
wantedcombination.byrate <- which.max(allmean)
combinations[wantedcombination.byrate]
```

**The code above is showing that ``r as.character(combinations[wantedcombination.byrate])`` is the best logistic regression.**

**In addition to Logistic regression, here I shall explore the results of LDA**

```{r}
lda.results <- lapply(combinations, function(x) lda(x, data = Weekly, subset = train))
lda.pred <- lapply(lda.results, function(x) predict(x, Weekly.test))
lda.class <- lapply(lda.pred, function(x) x$class)
lda.means <- lapply(lda.class, function(x) mean(x == Weekly.test$Direction))
max(unlist(lda.means))
which.max(lda.means)
```

**It seems that LDA gives similar results. The next few lines shows that we cannot make predictions with high certainty.**

```{r}
lda.numbers.50 <- lapply(lda.pred, function(x) sum(x$posterior[, 1] > 0.5))
max(unlist(lda.numbers.50))
lda.numbers.70 <- lapply(lda.pred, function(x) sum(x$posterior[, 1] > 0.7))
max(unlist(lda.numbers.70))
lda.numbers.90 <- lapply(lda.pred, function(x) sum(x$posterior[, 1] > 0.9))
max(unlist(lda.numbers.90))
```


