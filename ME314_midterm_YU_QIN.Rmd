---
title: "Midterm Assignemnt, ME314 2018"
output: html_document
---
 
![](images/lse-logo.jpg)

#### Summer School 2018 midsession examination  

# ME314 Introduction to Data Science and Big Data 

## Suitable for all candidates


### Instructions to candidates  

* Complete the assignment by adding your answers directly to the RMarkdown document, knitting the document, and submitting the HTML file to Moodle.   
* Time allowed: due 19:00 on Wednesday, 8th August 2018.  
* Submit the assignment via [Moodle](https://shortcourses.lse.ac.uk/course/view.php?id=158).


You will need to load the core library for the course textbook and libraries for LDA and KNN:
```{r}
library(ISLR)
library(MASS)
library(class)
```

This question should be answered using the `Weekly` data set, which is part of the `ISLR` package. This data contains 1,089 weekly stock returns for 21 years, from the beginning of 1990 to the end of 2010.

1.   Produce some numerical and graphical summaries of the `Weekly` data. Do there appear to be any patterns?

```{r}
data(Weekly)
names(Weekly)
dim(Weekly)
summary(Weekly)
#plot(Weekly)
pairs(Weekly)
cor(subset(Weekly, select = -Direction))
```

2.  Use the full data set to perform a logistic regression with `Direction` as the response and the five lag variables plus `Volume` as predictors. Use the summary function to print the results. 

```{r}
glm.fits <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly ,family = binomial)
summary(glm.fits)
# The following codes do not give '*'s:
#summary(glm.fits)$coefficients
#summary(glm.fits)$coefficients[, 4]
#coef(glm.fits)
```

  Do any of the predictors appear to be statistically significant? If so, which ones?

**`Lag2` seems to be statistically significant, since it has a relatively small p-value, while all the others' p-values are too big to give a clear evidence of a real association with `Direction`.**

3.  Compute the confusion matrix and overall fraction of correct predictions. 

```{r}
contrasts(Weekly$Direction)
glm.probs <- predict(glm.fits, type = "response")
#glm.probs[1:10]
glm.pred <- rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] <- "Up"
table(glm.pred, Weekly$Direction)
mean(glm.pred == Weekly$Direction)
```
     
  Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

**In this case, logistic regression correctly predicted the movement of the stock ``r sprintf("%.2f %%", 100*mean(glm.pred == Weekly$Direction))`` of the time. The confusion matrix, with the off-diagonals representing incorrect predictions including false positives and false negatives, is telling me that the training error is high.**

4.  Now fit the logistic regression model using a training data period from 1990 to 2008, with `Lag2` as the only predictor. 

```{r}
train <- (Weekly$Year < 2009)
Weekly.test <- Weekly[!train, ]
Weekly.train <- Weekly[train, ]
glm.fits.subset <- glm(Direction ~ Lag2, data = Weekly ,family = binomial, subset = train)
summary(glm.fits.subset)
#glm.fits.train <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly.train ,family = binomial)
#summary(glm.fits.train)
```


  Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).

```{r}
glm.probs.test <- predict(glm.fits.subset, Weekly.test, type = "response")
glm.pred.test <- rep("Down", length(glm.probs.test))
glm.pred.test[glm.probs.test > 0.5] <- "Up"
table(glm.pred.test, Weekly.test$Direction)
mean(glm.pred.test == Weekly.test$Direction)
```

**Here, logistic regression correctly predicted the movement of the stock ``r sprintf("%.2f %%", 100*mean(glm.pred.test == Weekly.test$Direction))`` of the time**

5.  Experiment with different combinations of predictors, including possible transformations and interactions, and classification methods. 

**Firstly, try all the combinations of predictors:**

```{r}
predictors <- names(Weekly)[2: 7]
# Getting all combinations of predictors:
boolcombinations <- expand.grid(rep(list(c(TRUE, FALSE)), length(predictors)))
boolcombinations <- boolcombinations[-(dim(boolcombinations)[1]), ]
combinations <- apply(boolcombinations, 1, function(x) as.formula(
  paste("Direction ~ ", paste(predictors[x], collapse = " + "), sep ="")
  ))
# Getting all glm:
allresults <- lapply(combinations, function(x) glm(x, data = Weekly ,family = binomial))
allresults[[1]]$aic  # Verify that this AIC has the same value as in Q2
min(as.numeric(lapply(allresults, function(x) x$aic)))
# Finding the best combination:
wantedcombination <- which.min(as.numeric(lapply(allresults, function(x) x$aic)))
combinations[wantedcombination]
```

**So, according to AIC, the best combination of predictors is ``r predictors[unlist(boolcombinations[wantedcombination, ])]``, for logistic regression.**

```{r}
allresults.train <- lapply(combinations, function(x) glm(x, data = Weekly ,family = binomial, subset = train))
allprobs <- lapply(allresults, function(x) predict(x, Weekly.test, type = "response"))
allpred <- vector("list", length = length(allprobs))
allpred <- lapply(allprobs, function(x) rep("Down", length(allprobs[1])))
```

  Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data.
